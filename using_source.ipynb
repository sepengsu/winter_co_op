{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import deprl\n",
    "import gym\n",
    "import sconegym\n",
    "import yaml\n",
    "import torch \n",
    "\n",
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "dir_path  = []\n",
    "dir_path.append(r\"C:\\Users\\PC\\Documents\\SCONE\\results\")\n",
    "dir_path.append(r\"C:\\Users\\na062\\Documents\\SCONE\\results\")\n",
    "if os.path.exists(dir_path[0]) or os.path.isdir(dir_path[0]):\n",
    "    dir_path = dir_path[0]\n",
    "elif os.path.exists(dir_path[1]) or os.path.isdir(dir_path[1]):\n",
    "    dir_path = dir_path[1]\n",
    "\n",
    "else:\n",
    "    dir_path = input(\"경로 입력\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### train 1622"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(r'config\\\\scone_walk_h1622.yaml','r') as y:\n",
    "    config = yaml.safe_load(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#config['tonic']['name'] = 'h1622_v1_lua_50_bal'\n",
    "config['tonic']['name'] = \"deprl_paper_1622\"\n",
    "#config['tonic']['name'] = \"walk_2190\"\n",
    "config['tonic']['environment'] = \"deprl.environments.Gym('sconewalk_h1622-v1')\"\n",
    "#config['tonic']['environment'] = \"deprl.environments.Gym('sconewalk_h2190-v1')\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m\u001b[32mCUDA detected, storing default tensors on it.\u001b[0m\n",
      "\u001b[1m\u001b[32mStochastic Switch-DEP. Paper version.\u001b[0m\n",
      "\u001b[1m\u001b[32m테스트 결과 평균 reward: 167.7656922340393\u001b[0m\n",
      "\u001b[1m\u001b[32mSaved\u001b[0m\n",
      "--------------------\n",
      "Iteration 1, Duration: 0분 38초\n",
      "End Time: 2024-01-16 12:35:58\n",
      "--------------------\n",
      "\u001b[1m\u001b[32mCUDA detected, storing default tensors on it.\u001b[0m\n",
      "\u001b[1m\u001b[32mStochastic Switch-DEP. Paper version.\u001b[0m\n",
      "\u001b[1m\u001b[32mFound earlier run, continuing training.\u001b[0m\n",
      "\n",
      "\u001b[1m\u001b[33mWarning: Logging new keys ['train/energy_buffer/avg_relabel_action_cost', 'critic/loss', 'critic/q', 'actor/policy_mean_loss', 'actor/policy_std_loss', 'actor/kl_mean_loss', 'actor/kl_std_loss', 'actor/alpha_mean_loss', 'actor/alpha_std_loss', 'actor/temperature_loss', 'actor/temperature', 'actor/alpha_mean', 'actor/alpha_std', 'actor/penalty_temperature']\u001b[0m\n",
      "\u001b[1m\u001b[32m테스트 결과 평균 reward: 204.0729019165039\u001b[0m\n",
      "\u001b[1m\u001b[32mSaved\u001b[0m\n",
      "--------------------\n",
      "Iteration 2, Duration: 1분 43초\n",
      "End Time: 2024-01-16 12:37:42\n",
      "--------------------\n",
      "\u001b[1m\u001b[32mCUDA detected, storing default tensors on it.\u001b[0m\n",
      "\u001b[1m\u001b[32mStochastic Switch-DEP. Paper version.\u001b[0m\n",
      "\u001b[1m\u001b[32mFound earlier run, continuing training.\u001b[0m\n",
      "\u001b[1m\u001b[32m테스트 결과 평균 reward: 263.5931611061096\u001b[0m\n",
      "\u001b[1m\u001b[32mSaved\u001b[0m\n",
      "--------------------\n",
      "Iteration 3, Duration: 1분 45초\n",
      "End Time: 2024-01-16 12:39:27\n",
      "--------------------\n",
      "\u001b[1m\u001b[32mCUDA detected, storing default tensors on it.\u001b[0m\n",
      "\u001b[1m\u001b[32mStochastic Switch-DEP. Paper version.\u001b[0m\n",
      "\u001b[1m\u001b[32mFound earlier run, continuing training.\u001b[0m\n",
      "\u001b[1m\u001b[32m테스트 결과 평균 reward: 269.97480325102805\u001b[0m\n",
      "\u001b[1m\u001b[32mSaved\u001b[0m\n",
      "--------------------\n",
      "Iteration 4, Duration: 1분 46초\n",
      "End Time: 2024-01-16 12:41:14\n",
      "--------------------\n",
      "\u001b[1m\u001b[32mCUDA detected, storing default tensors on it.\u001b[0m\n",
      "\u001b[1m\u001b[32mStochastic Switch-DEP. Paper version.\u001b[0m\n",
      "\u001b[1m\u001b[32mFound earlier run, continuing training.\u001b[0m\n",
      "\u001b[1m\u001b[32m테스트 결과 평균 reward: 319.1884620666504\u001b[0m\n",
      "\u001b[1m\u001b[32mSaved\u001b[0m\n",
      "--------------------\n",
      "Iteration 5, Duration: 1분 44초\n",
      "End Time: 2024-01-16 12:42:59\n",
      "--------------------\n",
      "\u001b[1m\u001b[32mCUDA detected, storing default tensors on it.\u001b[0m\n",
      "\u001b[1m\u001b[32mStochastic Switch-DEP. Paper version.\u001b[0m\n",
      "\u001b[1m\u001b[32mFound earlier run, continuing training.\u001b[0m\n",
      "\u001b[1m\u001b[32m테스트 결과 평균 reward: 317.06003346443174\u001b[0m\n",
      "\u001b[1m\u001b[32mSaved\u001b[0m\n",
      "--------------------\n",
      "Iteration 6, Duration: 1분 42초\n",
      "End Time: 2024-01-16 12:44:41\n",
      "--------------------\n",
      "\u001b[1m\u001b[32mCUDA detected, storing default tensors on it.\u001b[0m\n",
      "\u001b[1m\u001b[32mStochastic Switch-DEP. Paper version.\u001b[0m\n",
      "\u001b[1m\u001b[32mFound earlier run, continuing training.\u001b[0m\n",
      "\u001b[1m\u001b[32m테스트 결과 평균 reward: 342.0797365665436\u001b[0m\n",
      "\u001b[1m\u001b[32mSaved\u001b[0m\n",
      "--------------------\n",
      "Iteration 7, Duration: 1분 44초\n",
      "End Time: 2024-01-16 12:46:26\n",
      "--------------------\n",
      "\u001b[1m\u001b[32mCUDA detected, storing default tensors on it.\u001b[0m\n",
      "\u001b[1m\u001b[32mStochastic Switch-DEP. Paper version.\u001b[0m\n",
      "\u001b[1m\u001b[32mFound earlier run, continuing training.\u001b[0m\n",
      "\u001b[1m\u001b[32m테스트 결과 평균 reward: 356.1284674167633\u001b[0m\n",
      "\u001b[1m\u001b[32mSaved\u001b[0m\n",
      "--------------------\n",
      "Iteration 8, Duration: 1분 48초\n",
      "End Time: 2024-01-16 12:48:14\n",
      "--------------------\n",
      "\u001b[1m\u001b[32mCUDA detected, storing default tensors on it.\u001b[0m\n",
      "\u001b[1m\u001b[32mStochastic Switch-DEP. Paper version.\u001b[0m\n",
      "\u001b[1m\u001b[32mFound earlier run, continuing training.\u001b[0m\n",
      "\u001b[1m\u001b[32m테스트 결과 평균 reward: 387.91438349485395\u001b[0m\n",
      "\u001b[1m\u001b[32mSaved\u001b[0m\n",
      "--------------------\n",
      "Iteration 9, Duration: 1분 48초\n",
      "End Time: 2024-01-16 12:50:03\n",
      "--------------------\n",
      "\u001b[1m\u001b[32mCUDA detected, storing default tensors on it.\u001b[0m\n",
      "\u001b[1m\u001b[32mStochastic Switch-DEP. Paper version.\u001b[0m\n",
      "\u001b[1m\u001b[32mFound earlier run, continuing training.\u001b[0m\n",
      "\u001b[1m\u001b[32m테스트 결과 평균 reward: 428.12772884368894\u001b[0m\n",
      "\u001b[1m\u001b[32mSaved\u001b[0m\n",
      "--------------------\n",
      "Iteration 10, Duration: 1분 48초\n",
      "End Time: 2024-01-16 12:51:52\n",
      "--------------------\n",
      "\u001b[1m\u001b[32mCUDA detected, storing default tensors on it.\u001b[0m\n",
      "\u001b[1m\u001b[32mStochastic Switch-DEP. Paper version.\u001b[0m\n",
      "\u001b[1m\u001b[32mFound earlier run, continuing training.\u001b[0m\n",
      "\u001b[1m\u001b[32m테스트 결과 평균 reward: 428.771546459198\u001b[0m\n",
      "\u001b[1m\u001b[32mSaved\u001b[0m\n",
      "--------------------\n",
      "Iteration 11, Duration: 1분 48초\n",
      "End Time: 2024-01-16 12:53:40\n",
      "--------------------\n",
      "\u001b[1m\u001b[32mCUDA detected, storing default tensors on it.\u001b[0m\n",
      "\u001b[1m\u001b[32mStochastic Switch-DEP. Paper version.\u001b[0m\n",
      "\u001b[1m\u001b[32mFound earlier run, continuing training.\u001b[0m\n",
      "\u001b[1m\u001b[32m테스트 결과 평균 reward: 498.1538122177124\u001b[0m\n",
      "\u001b[1m\u001b[32mSaved\u001b[0m\n",
      "--------------------\n",
      "Iteration 12, Duration: 1분 49초\n",
      "End Time: 2024-01-16 12:55:29\n",
      "--------------------\n",
      "\u001b[1m\u001b[32mCUDA detected, storing default tensors on it.\u001b[0m\n",
      "\u001b[1m\u001b[32mStochastic Switch-DEP. Paper version.\u001b[0m\n",
      "\u001b[1m\u001b[32mFound earlier run, continuing training.\u001b[0m\n",
      "\u001b[1m\u001b[32m테스트 결과 평균 reward: 683.4728356838226\u001b[0m\n",
      "\u001b[1m\u001b[32mSaved\u001b[0m\n",
      "--------------------\n",
      "Iteration 13, Duration: 1분 49초\n",
      "End Time: 2024-01-16 12:57:18\n",
      "--------------------\n",
      "\u001b[1m\u001b[32mCUDA detected, storing default tensors on it.\u001b[0m\n",
      "\u001b[1m\u001b[32mStochastic Switch-DEP. Paper version.\u001b[0m\n",
      "\u001b[1m\u001b[32mFound earlier run, continuing training.\u001b[0m\n",
      "\u001b[1m\u001b[32m테스트 결과 평균 reward: 678.5288860440254\u001b[0m\n",
      "\u001b[1m\u001b[32mSaved\u001b[0m\n",
      "--------------------\n",
      "Iteration 14, Duration: 1분 48초\n",
      "End Time: 2024-01-16 12:59:07\n",
      "--------------------\n",
      "\u001b[1m\u001b[32mCUDA detected, storing default tensors on it.\u001b[0m\n",
      "\u001b[1m\u001b[32mStochastic Switch-DEP. Paper version.\u001b[0m\n",
      "\u001b[1m\u001b[32mFound earlier run, continuing training.\u001b[0m\n",
      "\u001b[1m\u001b[32m테스트 결과 평균 reward: 872.3847843050956\u001b[0m\n",
      "\u001b[1m\u001b[32mSaved\u001b[0m\n",
      "--------------------\n",
      "Iteration 15, Duration: 1분 50초\n",
      "End Time: 2024-01-16 13:00:58\n",
      "--------------------\n",
      "\u001b[1m\u001b[32mCUDA detected, storing default tensors on it.\u001b[0m\n",
      "\u001b[1m\u001b[32mStochastic Switch-DEP. Paper version.\u001b[0m\n",
      "\u001b[1m\u001b[32mFound earlier run, continuing training.\u001b[0m\n",
      "\u001b[1m\u001b[32m테스트 결과 평균 reward: 1213.6194615542656\u001b[0m\n",
      "\u001b[1m\u001b[32mSaved\u001b[0m\n",
      "--------------------\n",
      "Iteration 16, Duration: 1분 51초\n",
      "End Time: 2024-01-16 13:02:50\n",
      "--------------------\n",
      "\u001b[1m\u001b[32mCUDA detected, storing default tensors on it.\u001b[0m\n",
      "\u001b[1m\u001b[32mStochastic Switch-DEP. Paper version.\u001b[0m\n",
      "\u001b[1m\u001b[32mFound earlier run, continuing training.\u001b[0m\n",
      "\u001b[1m\u001b[32m테스트 결과 평균 reward: 2110.0411578670146\u001b[0m\n",
      "\u001b[1m\u001b[32mSaved\u001b[0m\n",
      "--------------------\n",
      "Iteration 17, Duration: 1분 56초\n",
      "End Time: 2024-01-16 13:04:46\n",
      "--------------------\n",
      "\u001b[1m\u001b[32mCUDA detected, storing default tensors on it.\u001b[0m\n",
      "\u001b[1m\u001b[32mStochastic Switch-DEP. Paper version.\u001b[0m\n",
      "\u001b[1m\u001b[32mFound earlier run, continuing training.\u001b[0m\n",
      "\u001b[1m\u001b[32m테스트 결과 평균 reward: 2126.4869185247285\u001b[0m\n",
      "\u001b[1m\u001b[32mSaved\u001b[0m\n",
      "--------------------\n",
      "Iteration 18, Duration: 1분 54초\n",
      "End Time: 2024-01-16 13:06:41\n",
      "--------------------\n",
      "\u001b[1m\u001b[32mCUDA detected, storing default tensors on it.\u001b[0m\n",
      "\u001b[1m\u001b[32mStochastic Switch-DEP. Paper version.\u001b[0m\n",
      "\u001b[1m\u001b[32mFound earlier run, continuing training.\u001b[0m\n",
      "\u001b[1m\u001b[32m테스트 결과 평균 reward: 2870.7334972184617\u001b[0m\n",
      "\u001b[1m\u001b[32mSaved\u001b[0m\n",
      "--------------------\n",
      "Iteration 19, Duration: 1분 54초\n",
      "End Time: 2024-01-16 13:08:35\n",
      "--------------------\n",
      "\u001b[1m\u001b[32mCUDA detected, storing default tensors on it.\u001b[0m\n",
      "\u001b[1m\u001b[32mStochastic Switch-DEP. Paper version.\u001b[0m\n",
      "\u001b[1m\u001b[32mFound earlier run, continuing training.\u001b[0m\n",
      "\u001b[1m\u001b[32m테스트 결과 평균 reward: 3876.3931466612385\u001b[0m\n",
      "\u001b[1m\u001b[32mSaved\u001b[0m\n",
      "--------------------\n",
      "Iteration 20, Duration: 1분 55초\n",
      "End Time: 2024-01-16 13:10:30\n",
      "--------------------\n",
      "\u001b[1m\u001b[32mCUDA detected, storing default tensors on it.\u001b[0m\n",
      "\u001b[1m\u001b[32mStochastic Switch-DEP. Paper version.\u001b[0m\n",
      "\u001b[1m\u001b[32mFound earlier run, continuing training.\u001b[0m\n",
      "\u001b[1m\u001b[32m테스트 결과 평균 reward: 3840.515057384176\u001b[0m\n",
      "\u001b[1m\u001b[32mSaved\u001b[0m\n",
      "--------------------\n",
      "Iteration 21, Duration: 1분 55초\n",
      "End Time: 2024-01-16 13:12:26\n",
      "--------------------\n",
      "\u001b[1m\u001b[32mCUDA detected, storing default tensors on it.\u001b[0m\n",
      "\u001b[1m\u001b[32mStochastic Switch-DEP. Paper version.\u001b[0m\n",
      "\u001b[1m\u001b[32mFound earlier run, continuing training.\u001b[0m\n",
      "\u001b[1m\u001b[32m테스트 결과 평균 reward: 5442.096629865468\u001b[0m\n",
      "\u001b[1m\u001b[32mSaved\u001b[0m\n",
      "--------------------\n",
      "Iteration 22, Duration: 1분 59초\n",
      "End Time: 2024-01-16 13:14:25\n",
      "--------------------\n",
      "\u001b[1m\u001b[32mCUDA detected, storing default tensors on it.\u001b[0m\n",
      "\u001b[1m\u001b[32mStochastic Switch-DEP. Paper version.\u001b[0m\n",
      "\u001b[1m\u001b[32mFound earlier run, continuing training.\u001b[0m\n",
      "\u001b[1m\u001b[32m테스트 결과 평균 reward: 7161.780605311552\u001b[0m\n",
      "\u001b[1m\u001b[32mSaved\u001b[0m\n",
      "--------------------\n",
      "Iteration 23, Duration: 1분 55초\n",
      "End Time: 2024-01-16 13:16:20\n",
      "--------------------\n",
      "\u001b[1m\u001b[32mCUDA detected, storing default tensors on it.\u001b[0m\n",
      "\u001b[1m\u001b[32mStochastic Switch-DEP. Paper version.\u001b[0m\n",
      "\u001b[1m\u001b[32mFound earlier run, continuing training.\u001b[0m\n",
      "\u001b[1m\u001b[32m테스트 결과 평균 reward: 7770.098138618469\u001b[0m\n",
      "\u001b[1m\u001b[32mSaved\u001b[0m\n",
      "--------------------\n",
      "Iteration 24, Duration: 1분 51초\n",
      "End Time: 2024-01-16 13:18:12\n",
      "--------------------\n",
      "\u001b[1m\u001b[32mCUDA detected, storing default tensors on it.\u001b[0m\n",
      "\u001b[1m\u001b[32mStochastic Switch-DEP. Paper version.\u001b[0m\n",
      "\u001b[1m\u001b[32mFound earlier run, continuing training.\u001b[0m\n",
      "\u001b[1m\u001b[32m테스트 결과 평균 reward: 7186.379402542114\u001b[0m\n",
      "\u001b[1m\u001b[32mSaved\u001b[0m\n",
      "--------------------\n",
      "Iteration 25, Duration: 1분 55초\n",
      "End Time: 2024-01-16 13:20:07\n",
      "--------------------\n",
      "\u001b[1m\u001b[32mCUDA detected, storing default tensors on it.\u001b[0m\n",
      "\u001b[1m\u001b[32mStochastic Switch-DEP. Paper version.\u001b[0m\n",
      "\u001b[1m\u001b[32mFound earlier run, continuing training.\u001b[0m\n",
      "\u001b[1m\u001b[32m테스트 결과 평균 reward: 8040.54710521698\u001b[0m\n",
      "\u001b[1m\u001b[32mSaved\u001b[0m\n",
      "--------------------\n",
      "Iteration 26, Duration: 1분 59초\n",
      "End Time: 2024-01-16 13:22:07\n",
      "--------------------\n",
      "\u001b[1m\u001b[32mCUDA detected, storing default tensors on it.\u001b[0m\n",
      "\u001b[1m\u001b[32mStochastic Switch-DEP. Paper version.\u001b[0m\n",
      "\u001b[1m\u001b[32mFound earlier run, continuing training.\u001b[0m\n",
      "\u001b[1m\u001b[32m테스트 결과 평균 reward: 8106.285279417038\u001b[0m\n",
      "\u001b[1m\u001b[32mSaved\u001b[0m\n",
      "--------------------\n",
      "Iteration 27, Duration: 2분 0초\n",
      "End Time: 2024-01-16 13:24:08\n",
      "--------------------\n",
      "\u001b[1m\u001b[32mCUDA detected, storing default tensors on it.\u001b[0m\n",
      "\u001b[1m\u001b[32mStochastic Switch-DEP. Paper version.\u001b[0m\n",
      "\u001b[1m\u001b[32mFound earlier run, continuing training.\u001b[0m\n",
      "\u001b[1m\u001b[32m테스트 결과 평균 reward: 8099.366982460022\u001b[0m\n",
      "\u001b[1m\u001b[32mSaved\u001b[0m\n",
      "--------------------\n",
      "Iteration 28, Duration: 1분 53초\n",
      "End Time: 2024-01-16 13:26:01\n",
      "--------------------\n",
      "\u001b[1m\u001b[32mCUDA detected, storing default tensors on it.\u001b[0m\n",
      "\u001b[1m\u001b[32mStochastic Switch-DEP. Paper version.\u001b[0m\n",
      "\u001b[1m\u001b[32mFound earlier run, continuing training.\u001b[0m\n",
      "\u001b[1m\u001b[32m테스트 결과 평균 reward: 8101.4758258342745\u001b[0m\n",
      "\u001b[1m\u001b[32mSaved\u001b[0m\n",
      "--------------------\n",
      "Iteration 29, Duration: 1분 52초\n",
      "End Time: 2024-01-16 13:27:53\n",
      "--------------------\n",
      "\u001b[1m\u001b[32mCUDA detected, storing default tensors on it.\u001b[0m\n",
      "\u001b[1m\u001b[32mStochastic Switch-DEP. Paper version.\u001b[0m\n",
      "\u001b[1m\u001b[32mFound earlier run, continuing training.\u001b[0m\n",
      "\u001b[1m\u001b[32m테스트 결과 평균 reward: 7811.559485673904\u001b[0m\n",
      "\u001b[1m\u001b[32mSaved\u001b[0m\n",
      "--------------------\n",
      "Iteration 30, Duration: 2분 0초\n",
      "End Time: 2024-01-16 13:29:54\n",
      "--------------------\n"
     ]
    }
   ],
   "source": [
    "from deprl import main\n",
    "import time\n",
    "for i in range(30):\n",
    "    # Set the custom trainer with increasing steps for each iteration\n",
    "    config['tonic']['trainer'] = f'deprl.custom_trainer.Trainer(steps=int(2e5)*{i+1}, epoch_steps=int(2e5), save_steps=int(2e5))'\n",
    "    \n",
    "    # Capture the start time\n",
    "    start_time = time.time()\n",
    "    \n",
    "    # Start the training process\n",
    "    main.main(config)\n",
    "    \n",
    "    # Capture the end time\n",
    "    end_time = time.time()\n",
    "    \n",
    "    # Calculate and print the duration\n",
    "    duration = end_time - start_time\n",
    "    minutes, seconds = divmod(duration, 60)\n",
    "    \n",
    "    print(\"-\" * 20)    \n",
    "    print(f\"Iteration {i+1}, Duration: {int(minutes)}분 {int(seconds)}초\")\n",
    "    print(f\"End Time: {time.strftime('%Y-%m-%d %H:%M:%S', time.localtime(end_time))}\")\n",
    "    print(\"-\" * 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "path = os.path.join(dir_path,config['tonic']['name'])\n",
    "names = os.listdir(path)[0]\n",
    "path = os.path.join(path,names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m\u001b[32mStochastic Switch-DEP. Paper version.\u001b[0m\n",
      "\u001b[1m\u001b[32mLoading\u001b[0m\n",
      "Episode 0 ending; steps=93; reward=6568.202;                 com=[ 1.264790 0.635384 -0.129344 ]\n",
      "Episode 1 ending; steps=88; reward=6933.065;                 com=[ 1.376284 0.609492 -0.031971 ]\n",
      "Episode 2 ending; steps=87; reward=6958.431;                 com=[ 1.326771 0.633686 0.259756 ]\n",
      "Episode 3 ending; steps=69; reward=4470.115;                 com=[ 0.912048 0.650032 0.179794 ]\n",
      "Episode 4 ending; steps=82; reward=6277.102;                 com=[ 1.277941 0.634415 0.332600 ]\n",
      "Episode 5 ending; steps=99; reward=7808.711;                 com=[ 1.694008 0.628146 0.154833 ]\n",
      "Episode 6 ending; steps=100; reward=7892.845;                 com=[ 1.690117 0.649946 0.347537 ]\n",
      "Episode 7 ending; steps=76; reward=4537.282;                 com=[ 1.067325 0.646648 -0.274340 ]\n",
      "Episode 8 ending; steps=88; reward=6161.589;                 com=[ 1.324898 0.649205 -0.406598 ]\n",
      "Episode 9 ending; steps=77; reward=6096.806;                 com=[ 1.212275 0.616736 -0.261412 ]\n"
     ]
    }
   ],
   "source": [
    "env = eval(config['tonic']['environment'])\n",
    "policy = deprl.utils.load(os.path.join(path,'checkpoints'),env)\n",
    "env.seed(0)\n",
    "for ep in range(10):\n",
    "    ep_steps = 0\n",
    "    ep_tot_reward = 0\n",
    "    state = env.reset()\n",
    "    if ep % 1 == 0:\n",
    "        env.store_next_episode()  # Store results of every Nth episode\n",
    "\n",
    "    while True:\n",
    "        # samples random action\n",
    "        action = policy(state)\n",
    "        # applies action and advances environment by one step\n",
    "        state, reward, done, info = env.step(action)\n",
    "        ep_steps += 1\n",
    "        ep_tot_reward += reward\n",
    "\n",
    "        # check if done\n",
    "        if done or (ep_steps >= 1000):\n",
    "            print(\n",
    "                f\"Episode {ep} ending; steps={ep_steps}; reward={ep_tot_reward:0.3f}; \\\n",
    "                com={env.model.com_pos()}\"\n",
    "            )\n",
    "            env.write_now()\n",
    "            env.reset()\n",
    "            break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### temp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m\u001b[32mLoading experiment from C:\\\\Users\\\\na062\\\\Documents\\\\SCONE\\\\results\\sconewalk_h1622_v1\\231228.094659.H1622v2k\\checkpoints\u001b[0m\n",
      "\u001b[1m\u001b[32mStochastic Switch-DEP. Paper version.\u001b[0m\n",
      "\u001b[1m\u001b[32m\n",
      "Loading weights from C:\\\\Users\\\\na062\\\\Documents\\\\SCONE\\\\results\\sconewalk_h1622_v1\\231228.094659.H1622v2k\\checkpoints\\step_5000000.pt\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "env = eval(config['tonic']['environment'])\n",
    "policy = deprl.utils.load(os.path.join(path,'checkpoints'),env)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python_scone",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
