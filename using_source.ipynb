{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import deprl\n",
    "import gym\n",
    "import sconegym\n",
    "import yaml\n",
    "import torch \n",
    "\n",
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "dir_path  = []\n",
    "dir_path.append(r\"C:\\Users\\PC\\Documents\\SCONE\\results\")\n",
    "dir_path.append(r\"C:\\Users\\na062\\Documents\\SCONE\\results\")\n",
    "if os.path.exists(dir_path[0]) or os.path.isdir(dir_path[0]):\n",
    "    dir_path = dir_path[0]\n",
    "elif os.path.exists(dir_path[1]) or os.path.isdir(dir_path[1]):\n",
    "    dir_path = dir_path[1]\n",
    "\n",
    "else:\n",
    "    dir_path = input(\"경로 입력\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### train 1622"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(r'config\\\\scone_walk_h1622.yaml','r') as y:\n",
    "    config = yaml.safe_load(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "config['tonic']['name'] = 'h1622_v1_lua_50_bal'\n",
    "config['tonic']['environment'] = \"deprl.environments.Gym('sconewalk_h1622-v1')\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m\u001b[32mCUDA detected, storing default tensors on it.\u001b[0m\n",
      "\u001b[1m\u001b[32mStochastic Switch-DEP. Paper version.\u001b[0m\n",
      "\u001b[1m\u001b[32mFound earlier run, continuing training.\u001b[0m\n",
      "\u001b[1m\u001b[32m테스트 결과 평균 reward: 5303.059358215332\u001b[0m\n",
      "\u001b[1m\u001b[32mSaved\u001b[0m\n",
      "--------------------\n",
      "Iteration 21, Duration: 1분 48초\n",
      "End Time: 2024-01-15 11:11:52\n",
      "--------------------\n",
      "\u001b[1m\u001b[32mCUDA detected, storing default tensors on it.\u001b[0m\n",
      "\u001b[1m\u001b[32mStochastic Switch-DEP. Paper version.\u001b[0m\n",
      "\u001b[1m\u001b[32mFound earlier run, continuing training.\u001b[0m\n",
      "\u001b[1m\u001b[32m테스트 결과 평균 reward: 4383.692926025391\u001b[0m\n",
      "\u001b[1m\u001b[32mSaved\u001b[0m\n",
      "--------------------\n",
      "Iteration 22, Duration: 1분 39초\n",
      "End Time: 2024-01-15 11:13:31\n",
      "--------------------\n",
      "\u001b[1m\u001b[32mCUDA detected, storing default tensors on it.\u001b[0m\n",
      "\u001b[1m\u001b[32mStochastic Switch-DEP. Paper version.\u001b[0m\n",
      "\u001b[1m\u001b[32mFound earlier run, continuing training.\u001b[0m\n",
      "\u001b[1m\u001b[32m테스트 결과 평균 reward: 4809.3747261047365\u001b[0m\n",
      "\u001b[1m\u001b[32mSaved\u001b[0m\n",
      "--------------------\n",
      "Iteration 23, Duration: 1분 38초\n",
      "End Time: 2024-01-15 11:15:10\n",
      "--------------------\n",
      "\u001b[1m\u001b[32mCUDA detected, storing default tensors on it.\u001b[0m\n",
      "\u001b[1m\u001b[32mStochastic Switch-DEP. Paper version.\u001b[0m\n",
      "\u001b[1m\u001b[32mFound earlier run, continuing training.\u001b[0m\n",
      "\u001b[1m\u001b[32m테스트 결과 평균 reward: 4700.175675201416\u001b[0m\n",
      "\u001b[1m\u001b[32mSaved\u001b[0m\n",
      "--------------------\n",
      "Iteration 24, Duration: 1분 40초\n",
      "End Time: 2024-01-15 11:16:51\n",
      "--------------------\n",
      "\u001b[1m\u001b[32mCUDA detected, storing default tensors on it.\u001b[0m\n",
      "\u001b[1m\u001b[32mStochastic Switch-DEP. Paper version.\u001b[0m\n",
      "\u001b[1m\u001b[32mFound earlier run, continuing training.\u001b[0m\n",
      "\u001b[1m\u001b[32m테스트 결과 평균 reward: 5406.865771484375\u001b[0m\n",
      "\u001b[1m\u001b[32mSaved\u001b[0m\n",
      "--------------------\n",
      "Iteration 25, Duration: 1분 46초\n",
      "End Time: 2024-01-15 11:18:37\n",
      "--------------------\n",
      "\u001b[1m\u001b[32mCUDA detected, storing default tensors on it.\u001b[0m\n",
      "\u001b[1m\u001b[32mStochastic Switch-DEP. Paper version.\u001b[0m\n",
      "\u001b[1m\u001b[32mFound earlier run, continuing training.\u001b[0m\n",
      "\u001b[1m\u001b[32m테스트 결과 평균 reward: 5772.6358665466305\u001b[0m\n",
      "\u001b[1m\u001b[32mSaved\u001b[0m\n",
      "--------------------\n",
      "Iteration 26, Duration: 1분 46초\n",
      "End Time: 2024-01-15 11:20:24\n",
      "--------------------\n",
      "\u001b[1m\u001b[32mCUDA detected, storing default tensors on it.\u001b[0m\n",
      "\u001b[1m\u001b[32mStochastic Switch-DEP. Paper version.\u001b[0m\n",
      "\u001b[1m\u001b[32mFound earlier run, continuing training.\u001b[0m\n",
      "\u001b[1m\u001b[32m테스트 결과 평균 reward: 5673.0488182067875\u001b[0m\n",
      "\u001b[1m\u001b[32mSaved\u001b[0m\n",
      "--------------------\n",
      "Iteration 27, Duration: 1분 40초\n",
      "End Time: 2024-01-15 11:22:05\n",
      "--------------------\n",
      "\u001b[1m\u001b[32mCUDA detected, storing default tensors on it.\u001b[0m\n",
      "\u001b[1m\u001b[32mStochastic Switch-DEP. Paper version.\u001b[0m\n",
      "\u001b[1m\u001b[32mFound earlier run, continuing training.\u001b[0m\n",
      "\u001b[1m\u001b[32m테스트 결과 평균 reward: 5614.163679504394\u001b[0m\n",
      "\u001b[1m\u001b[32mSaved\u001b[0m\n",
      "--------------------\n",
      "Iteration 28, Duration: 1분 40초\n",
      "End Time: 2024-01-15 11:23:45\n",
      "--------------------\n",
      "\u001b[1m\u001b[32mCUDA detected, storing default tensors on it.\u001b[0m\n",
      "\u001b[1m\u001b[32mStochastic Switch-DEP. Paper version.\u001b[0m\n",
      "\u001b[1m\u001b[32mFound earlier run, continuing training.\u001b[0m\n",
      "\u001b[1m\u001b[32m테스트 결과 평균 reward: 6053.532183074951\u001b[0m\n",
      "\u001b[1m\u001b[32mSaved\u001b[0m\n",
      "--------------------\n",
      "Iteration 29, Duration: 1분 40초\n",
      "End Time: 2024-01-15 11:25:25\n",
      "--------------------\n",
      "\u001b[1m\u001b[32mCUDA detected, storing default tensors on it.\u001b[0m\n",
      "\u001b[1m\u001b[32mStochastic Switch-DEP. Paper version.\u001b[0m\n",
      "\u001b[1m\u001b[32mFound earlier run, continuing training.\u001b[0m\n",
      "\u001b[1m\u001b[32m테스트 결과 평균 reward: 5939.798816680908\u001b[0m\n",
      "\u001b[1m\u001b[32mSaved\u001b[0m\n",
      "--------------------\n",
      "Iteration 30, Duration: 1분 40초\n",
      "End Time: 2024-01-15 11:27:06\n",
      "--------------------\n",
      "\u001b[1m\u001b[32mCUDA detected, storing default tensors on it.\u001b[0m\n",
      "\u001b[1m\u001b[32mStochastic Switch-DEP. Paper version.\u001b[0m\n",
      "\u001b[1m\u001b[32mFound earlier run, continuing training.\u001b[0m\n",
      "\u001b[1m\u001b[32m테스트 결과 평균 reward: 5958.970653533936\u001b[0m\n",
      "\u001b[1m\u001b[32mSaved\u001b[0m\n",
      "--------------------\n",
      "Iteration 31, Duration: 1분 45초\n",
      "End Time: 2024-01-15 11:28:52\n",
      "--------------------\n",
      "\u001b[1m\u001b[32mCUDA detected, storing default tensors on it.\u001b[0m\n",
      "\u001b[1m\u001b[32mStochastic Switch-DEP. Paper version.\u001b[0m\n",
      "\u001b[1m\u001b[32mFound earlier run, continuing training.\u001b[0m\n",
      "\u001b[1m\u001b[32m테스트 결과 평균 reward: 5875.490977478027\u001b[0m\n",
      "\u001b[1m\u001b[32mSaved\u001b[0m\n",
      "--------------------\n",
      "Iteration 32, Duration: 1분 41초\n",
      "End Time: 2024-01-15 11:30:33\n",
      "--------------------\n",
      "\u001b[1m\u001b[32mCUDA detected, storing default tensors on it.\u001b[0m\n",
      "\u001b[1m\u001b[32mStochastic Switch-DEP. Paper version.\u001b[0m\n",
      "\u001b[1m\u001b[32mFound earlier run, continuing training.\u001b[0m\n",
      "\u001b[1m\u001b[32m테스트 결과 평균 reward: 5983.9900177001955\u001b[0m\n",
      "\u001b[1m\u001b[32mSaved\u001b[0m\n",
      "--------------------\n",
      "Iteration 33, Duration: 1분 41초\n",
      "End Time: 2024-01-15 11:32:15\n",
      "--------------------\n",
      "\u001b[1m\u001b[32mCUDA detected, storing default tensors on it.\u001b[0m\n",
      "\u001b[1m\u001b[32mStochastic Switch-DEP. Paper version.\u001b[0m\n",
      "\u001b[1m\u001b[32mFound earlier run, continuing training.\u001b[0m\n",
      "\u001b[1m\u001b[32m테스트 결과 평균 reward: 5357.669976043701\u001b[0m\n",
      "\u001b[1m\u001b[32mSaved\u001b[0m\n",
      "--------------------\n",
      "Iteration 34, Duration: 1분 40초\n",
      "End Time: 2024-01-15 11:33:55\n",
      "--------------------\n",
      "\u001b[1m\u001b[32mCUDA detected, storing default tensors on it.\u001b[0m\n",
      "\u001b[1m\u001b[32mStochastic Switch-DEP. Paper version.\u001b[0m\n",
      "\u001b[1m\u001b[32mFound earlier run, continuing training.\u001b[0m\n",
      "\u001b[1m\u001b[32m테스트 결과 평균 reward: 6188.278825378418\u001b[0m\n",
      "\u001b[1m\u001b[32mSaved\u001b[0m\n",
      "--------------------\n",
      "Iteration 35, Duration: 1분 42초\n",
      "End Time: 2024-01-15 11:35:38\n",
      "--------------------\n",
      "\u001b[1m\u001b[32mCUDA detected, storing default tensors on it.\u001b[0m\n",
      "\u001b[1m\u001b[32mStochastic Switch-DEP. Paper version.\u001b[0m\n",
      "\u001b[1m\u001b[32mFound earlier run, continuing training.\u001b[0m\n",
      "\u001b[1m\u001b[32m테스트 결과 평균 reward: 5529.308534240723\u001b[0m\n",
      "\u001b[1m\u001b[32mSaved\u001b[0m\n",
      "--------------------\n",
      "Iteration 36, Duration: 1분 41초\n",
      "End Time: 2024-01-15 11:37:19\n",
      "--------------------\n",
      "\u001b[1m\u001b[32mCUDA detected, storing default tensors on it.\u001b[0m\n",
      "\u001b[1m\u001b[32mStochastic Switch-DEP. Paper version.\u001b[0m\n",
      "\u001b[1m\u001b[32mFound earlier run, continuing training.\u001b[0m\n",
      "\u001b[1m\u001b[32m테스트 결과 평균 reward: 5649.2425315856935\u001b[0m\n",
      "\u001b[1m\u001b[32mSaved\u001b[0m\n",
      "--------------------\n",
      "Iteration 37, Duration: 1분 46초\n",
      "End Time: 2024-01-15 11:39:06\n",
      "--------------------\n",
      "\u001b[1m\u001b[32mCUDA detected, storing default tensors on it.\u001b[0m\n",
      "\u001b[1m\u001b[32mStochastic Switch-DEP. Paper version.\u001b[0m\n",
      "\u001b[1m\u001b[32mFound earlier run, continuing training.\u001b[0m\n",
      "\u001b[1m\u001b[32m테스트 결과 평균 reward: 5403.967910003662\u001b[0m\n",
      "\u001b[1m\u001b[32mSaved\u001b[0m\n",
      "--------------------\n",
      "Iteration 38, Duration: 1분 46초\n",
      "End Time: 2024-01-15 11:40:52\n",
      "--------------------\n",
      "\u001b[1m\u001b[32mCUDA detected, storing default tensors on it.\u001b[0m\n",
      "\u001b[1m\u001b[32mStochastic Switch-DEP. Paper version.\u001b[0m\n",
      "\u001b[1m\u001b[32mFound earlier run, continuing training.\u001b[0m\n",
      "\u001b[1m\u001b[32m테스트 결과 평균 reward: 5794.225661468506\u001b[0m\n",
      "\u001b[1m\u001b[32mSaved\u001b[0m\n",
      "--------------------\n",
      "Iteration 39, Duration: 1분 45초\n",
      "End Time: 2024-01-15 11:42:38\n",
      "--------------------\n",
      "\u001b[1m\u001b[32mCUDA detected, storing default tensors on it.\u001b[0m\n",
      "\u001b[1m\u001b[32mStochastic Switch-DEP. Paper version.\u001b[0m\n",
      "\u001b[1m\u001b[32mFound earlier run, continuing training.\u001b[0m\n",
      "\u001b[1m\u001b[32m테스트 결과 평균 reward: 5392.441535186767\u001b[0m\n",
      "\u001b[1m\u001b[32mSaved\u001b[0m\n",
      "--------------------\n",
      "Iteration 40, Duration: 1분 46초\n",
      "End Time: 2024-01-15 11:44:24\n",
      "--------------------\n"
     ]
    }
   ],
   "source": [
    "from deprl import main\n",
    "import time\n",
    "for i in range(20,40):\n",
    "    # Set the custom trainer with increasing steps for each iteration\n",
    "    config['tonic']['trainer'] = f'deprl.custom_trainer.Trainer(steps=int(2e5)*{i+1}, epoch_steps=int(2e5), save_steps=int(2e5))'\n",
    "    \n",
    "    # Capture the start time\n",
    "    start_time = time.time()\n",
    "    \n",
    "    # Start the training process\n",
    "    main.main(config)\n",
    "    \n",
    "    # Capture the end time\n",
    "    end_time = time.time()\n",
    "    \n",
    "    # Calculate and print the duration\n",
    "    duration = end_time - start_time\n",
    "    minutes, seconds = divmod(duration, 60)\n",
    "    \n",
    "    print(\"-\" * 20)    \n",
    "    print(f\"Iteration {i+1}, Duration: {int(minutes)}분 {int(seconds)}초\")\n",
    "    print(f\"End Time: {time.strftime('%Y-%m-%d %H:%M:%S', time.localtime(end_time))}\")\n",
    "    print(\"-\" * 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "path = os.path.join(dir_path,config['tonic']['name'])\n",
    "names = os.listdir(path)[0]\n",
    "path = os.path.join(path,names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m\u001b[32mStochastic Switch-DEP. Paper version.\u001b[0m\n",
      "\u001b[1m\u001b[32mLoading\u001b[0m\n",
      "Episode 0 ending; steps=93; reward=6568.202;                 com=[ 1.264790 0.635384 -0.129344 ]\n",
      "Episode 1 ending; steps=88; reward=6933.065;                 com=[ 1.376284 0.609492 -0.031971 ]\n",
      "Episode 2 ending; steps=87; reward=6958.431;                 com=[ 1.326771 0.633686 0.259756 ]\n",
      "Episode 3 ending; steps=69; reward=4470.115;                 com=[ 0.912048 0.650032 0.179794 ]\n",
      "Episode 4 ending; steps=82; reward=6277.102;                 com=[ 1.277941 0.634415 0.332600 ]\n",
      "Episode 5 ending; steps=99; reward=7808.711;                 com=[ 1.694008 0.628146 0.154833 ]\n",
      "Episode 6 ending; steps=100; reward=7892.845;                 com=[ 1.690117 0.649946 0.347537 ]\n",
      "Episode 7 ending; steps=76; reward=4537.282;                 com=[ 1.067325 0.646648 -0.274340 ]\n",
      "Episode 8 ending; steps=88; reward=6161.589;                 com=[ 1.324898 0.649205 -0.406598 ]\n",
      "Episode 9 ending; steps=77; reward=6096.806;                 com=[ 1.212275 0.616736 -0.261412 ]\n"
     ]
    }
   ],
   "source": [
    "env = eval(config['tonic']['environment'])\n",
    "policy = deprl.utils.load(os.path.join(path,'checkpoints'),env)\n",
    "env.seed(0)\n",
    "for ep in range(10):\n",
    "    ep_steps = 0\n",
    "    ep_tot_reward = 0\n",
    "    state = env.reset()\n",
    "    if ep % 1 == 0:\n",
    "        env.store_next_episode()  # Store results of every Nth episode\n",
    "\n",
    "    while True:\n",
    "        # samples random action\n",
    "        action = policy(state)\n",
    "        # applies action and advances environment by one step\n",
    "        state, reward, done, info = env.step(action)\n",
    "        ep_steps += 1\n",
    "        ep_tot_reward += reward\n",
    "\n",
    "        # check if done\n",
    "        if done or (ep_steps >= 1000):\n",
    "            print(\n",
    "                f\"Episode {ep} ending; steps={ep_steps}; reward={ep_tot_reward:0.3f}; \\\n",
    "                com={env.model.com_pos()}\"\n",
    "            )\n",
    "            env.write_now()\n",
    "            env.reset()\n",
    "            break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### temp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m\u001b[32mLoading experiment from C:\\\\Users\\\\na062\\\\Documents\\\\SCONE\\\\results\\sconewalk_h1622_v1\\231228.094659.H1622v2k\\checkpoints\u001b[0m\n",
      "\u001b[1m\u001b[32mStochastic Switch-DEP. Paper version.\u001b[0m\n",
      "\u001b[1m\u001b[32m\n",
      "Loading weights from C:\\\\Users\\\\na062\\\\Documents\\\\SCONE\\\\results\\sconewalk_h1622_v1\\231228.094659.H1622v2k\\checkpoints\\step_5000000.pt\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "env = eval(config['tonic']['environment'])\n",
    "policy = deprl.utils.load(os.path.join(path,'checkpoints'),env)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python_scone",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
