DEP:
  bias_rate: 0.002
  buffer_size: 200
  intervention_length: 8
  intervention_proba: 0.00371
  kappa: 1000
  normalization: independent
  q_norm_selector: l2
  regularization: 32
  s4avg: 2
  sensor_delay: 1
  tau: 40
  test_episode_every: 3
  time_dist: 5
  with_learning: true
env_args:
  clip_actions: true
  grf_coeff: -0.17281
  init_activations_mean: 0.01
  init_activations_std: 0
  joint_limit_coeff: -0.1307
  nmuscle_coeff: -1.57929
  smooth_coeff: -0.097
  step_size: 0.025
  vel_coeff: 10
tonic:
  after_training: ''
  agent: deprl.custom_agents.dep_factory(0,MyMPO())(replay=deprl.custom_replay_buffers.AdaptiveEnergyBuffer(return_steps=1,
    batch_size=256, steps_between_batches=1000, batch_iterations=30, steps_before_batches=2e5,
    num_acts=25))
  before_training: ''
  checkpoint: last
  environment: 'myutils.environments.Gym(''sconewalk_h1922-v1'', rwd_type_weights={''balance'':
    3},rwd_weights={''balance_velocity_zcom'': -0.3, ''balance_velcotiy_zhead'': -0.7})'
  full_save: 1
  header: import deprl, gym, sconegym, myutils; from myutils.my1922.custom_mpo_torch
    import MyMPO
  name: change_gym_100n
  parallel: 20
  resume: true
  seed: 0
  sequential: 10
  test_environment: null
  trainer: MyTrainer(steps=int(2e5)*500, epoch_steps=int(2e5), save_steps=int(1e6))
weights:
  reward:
    balance_velcotiy_zhead: -0.7
    balance_velocity_zcom: -0.3
  type:
    balance: 3
working_dir: IGNORED_FOR_HYFYDY
